{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from helpers import load_csv_data, create_csv_submission\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from plots import gradient_descent_visualization\n",
    "from implementations import mean_squared_error_gd, mean_squared_error_sgd\n",
    "from implementations import standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (328135, 321)\n",
      "x_test shape:  (109379, 321)\n",
      "y_train shape:  (328135,)\n",
      "train_ids shape:  (328135,)\n",
      "test_ids shape:  (109379,)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the dataset\n",
    "dir_path = './dataset_to_release/'\n",
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(dir_path)\n",
    "\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"train_ids shape: \", train_ids.shape)\n",
    "print(\"test_ids shape: \", test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN values: 328135\n"
     ]
    }
   ],
   "source": [
    "#print(x_train[0])\n",
    "#print(x_test[0])\n",
    "\n",
    "mask = np.ma.fix_invalid(x_train).mask\n",
    "\n",
    "# Count the number of rows with NaN values\n",
    "nan_cols_count = np.any(mask, axis=0).sum()\n",
    "\n",
    "# Print the number of rows with NaN values\n",
    "print(f\"Number of cols with NaN values: {nan_cols_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "new_arr = x_train[~np.ma.fix_invalid(x_train).mask.any(axis=1)]\n",
    "print(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.4362964   1.33036073  1.31787257  1.31224983  0.18042695 -0.15933182\n",
      " -0.42032874  2.53290043  2.53290043         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan -1.666927           nan\n",
      " -0.1981366  -0.47465744         nan         nan         nan         nan\n",
      " -0.26402971         nan         nan  0.09410313  0.26755222         nan\n",
      "         nan  0.23134955  0.12580625         nan -1.5343582   0.05831809\n",
      "         nan         nan  0.85611713 -0.74837655  0.07111893 -0.45623424\n",
      "         nan         nan         nan         nan -1.02006198         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "  1.01723124         nan         nan  1.43978406         nan -0.62587683\n",
      " -0.00355072         nan         nan         nan         nan -0.57388008\n",
      "         nan -0.30729497 -0.36631239 -0.77390404 -0.66972702 -0.34240844\n",
      "         nan -0.28804299 -0.20306793  0.22431364         nan -0.1678059\n",
      " -0.17437949  0.02294592 -0.44760941 -0.32830217 -0.44450071         nan\n",
      "  0.06013196 -0.72619879  0.09724365  0.36126153         nan         nan\n",
      "         nan         nan         nan -0.61059259 -0.47201587  0.03911607\n",
      "  0.13455567 -1.744085    0.3390155   0.09393662 -0.30779364 -0.30883082\n",
      " -0.25884183 -0.27446458         nan         nan         nan         nan\n",
      "         nan         nan -0.31766425 -0.32916536  0.32937713  0.35977195\n",
      "         nan         nan -0.05303001 -0.43774417  0.01460727  0.01090264\n",
      " -0.32938057 -0.35977282 -0.42050254         nan         nan -0.12338359\n",
      " -0.11255824         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan -0.32575929         nan\n",
      "         nan         nan         nan         nan         nan -0.5077313\n",
      " -0.55197761 -0.65349381 -0.34108858 -0.2359884  -0.652208   -1.038912\n",
      " -0.71065539         nan         nan         nan -0.34832576 -0.37611492\n",
      "         nan         nan         nan]\n",
      "[ 0.87528146 -1.24855161 -1.28434553 -1.26456552 -0.77976194 -0.15951222\n",
      " -0.41978502 -1.19162184 -1.19162184         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan -0.51310856  0.73531981  0.65117495         nan\n",
      " -0.19828    -0.47522212  0.20559551 -0.45864458 -1.17148174         nan\n",
      " -0.26569996         nan         nan  0.09219603  0.27467201         nan\n",
      "  0.21150027  0.22384506  0.12582599  0.48321826  0.30290767  0.06021182\n",
      "         nan         nan  0.85835954  0.44035529  1.00219651 -0.45270389\n",
      "         nan         nan         nan         nan  1.07330269         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      " -0.96592644         nan         nan  0.87233138         nan -0.63215986\n",
      " -0.68037898         nan         nan         nan         nan  1.28533917\n",
      "         nan -0.54993252 -0.36525018  1.3346653   0.9038982  -0.3437232\n",
      "         nan -0.29242657 -0.20459368  0.23457765         nan -0.16856643\n",
      " -0.17511757  0.0252045  -0.45185226 -0.33228607 -0.44932912         nan\n",
      "  1.4968097   1.25237084  1.45069789  1.00621592         nan         nan\n",
      "         nan         nan         nan -0.61037931 -0.47465597  0.99254948\n",
      "  1.7916975   0.30946239 -0.29221202  0.09288202 -0.30753287 -0.31042192\n",
      " -0.25974519 -0.27548194         nan         nan         nan         nan\n",
      "         nan         nan -0.32066265 -0.33158775  0.33274214  0.36222982\n",
      "         nan         nan -0.48807055 -0.44002629  0.01318098  0.00855252\n",
      " -0.33274437 -0.36223014 -0.42132581         nan         nan -0.23897017\n",
      " -0.12988747         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan -0.32649448         nan\n",
      "         nan         nan         nan         nan         nan -0.91116099\n",
      " -0.55093905 -0.65242536 -0.74699348 -0.63848334 -0.20249392 -0.6197895\n",
      " -0.30103285         nan         nan         nan -0.34906238 -0.37766012\n",
      "         nan         nan         nan]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Standardize the data\n",
    "x_train = standardize(x_train)\n",
    "x_test = standardize(x_test)\n",
    "\n",
    "print(x_train[0])\n",
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.zeros(x_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent(0/49): loss=nan, w0=nan\n",
      "Gradient Descent(1/49): loss=nan, w0=nan\n",
      "Gradient Descent(2/49): loss=nan, w0=nan\n",
      "Gradient Descent(3/49): loss=nan, w0=nan\n",
      "Gradient Descent(4/49): loss=nan, w0=nan\n",
      "Gradient Descent(5/49): loss=nan, w0=nan\n",
      "Gradient Descent(6/49): loss=nan, w0=nan\n",
      "Gradient Descent(7/49): loss=nan, w0=nan\n",
      "Gradient Descent(8/49): loss=nan, w0=nan\n",
      "Gradient Descent(9/49): loss=nan, w0=nan\n",
      "Gradient Descent(10/49): loss=nan, w0=nan\n",
      "Gradient Descent(11/49): loss=nan, w0=nan\n",
      "Gradient Descent(12/49): loss=nan, w0=nan\n",
      "Gradient Descent(13/49): loss=nan, w0=nan\n",
      "Gradient Descent(14/49): loss=nan, w0=nan\n",
      "Gradient Descent(15/49): loss=nan, w0=nan\n",
      "Gradient Descent(16/49): loss=nan, w0=nan\n",
      "Gradient Descent(17/49): loss=nan, w0=nan\n",
      "Gradient Descent(18/49): loss=nan, w0=nan\n",
      "Gradient Descent(19/49): loss=nan, w0=nan\n",
      "Gradient Descent(20/49): loss=nan, w0=nan\n",
      "Gradient Descent(21/49): loss=nan, w0=nan\n",
      "Gradient Descent(22/49): loss=nan, w0=nan\n",
      "Gradient Descent(23/49): loss=nan, w0=nan\n",
      "Gradient Descent(24/49): loss=nan, w0=nan\n",
      "Gradient Descent(25/49): loss=nan, w0=nan\n",
      "Gradient Descent(26/49): loss=nan, w0=nan\n",
      "Gradient Descent(27/49): loss=nan, w0=nan\n",
      "Gradient Descent(28/49): loss=nan, w0=nan\n",
      "Gradient Descent(29/49): loss=nan, w0=nan\n",
      "Gradient Descent(30/49): loss=nan, w0=nan\n",
      "Gradient Descent(31/49): loss=nan, w0=nan\n",
      "Gradient Descent(32/49): loss=nan, w0=nan\n",
      "Gradient Descent(33/49): loss=nan, w0=nan\n",
      "Gradient Descent(34/49): loss=nan, w0=nan\n",
      "Gradient Descent(35/49): loss=nan, w0=nan\n",
      "Gradient Descent(36/49): loss=nan, w0=nan\n",
      "Gradient Descent(37/49): loss=nan, w0=nan\n",
      "Gradient Descent(38/49): loss=nan, w0=nan\n",
      "Gradient Descent(39/49): loss=nan, w0=nan\n",
      "Gradient Descent(40/49): loss=nan, w0=nan\n",
      "Gradient Descent(41/49): loss=nan, w0=nan\n",
      "Gradient Descent(42/49): loss=nan, w0=nan\n",
      "Gradient Descent(43/49): loss=nan, w0=nan\n",
      "Gradient Descent(44/49): loss=nan, w0=nan\n",
      "Gradient Descent(45/49): loss=nan, w0=nan\n",
      "Gradient Descent(46/49): loss=nan, w0=nan\n",
      "Gradient Descent(47/49): loss=nan, w0=nan\n",
      "Gradient Descent(48/49): loss=nan, w0=nan\n",
      "Gradient Descent(49/49): loss=nan, w0=nan\n",
      "Gradient Descent: loss=nan, w=[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      "GD: execution time=18.504 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "w, loss = mean_squared_error_gd(y_train, x_train, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "print(\"Gradient Descent: loss={l}, w={w}\".format(l=loss, w=w))\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gd_ws' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m\n\u001b[0;32m      6\u001b[0m     fig \u001b[38;5;241m=\u001b[39m gradient_descent_visualization(\n\u001b[0;32m      7\u001b[0m         gd_losses,\n\u001b[0;32m      8\u001b[0m         gd_ws,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m         n_iter,\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     18\u001b[0m     fig\u001b[38;5;241m.\u001b[39mset_size_inches(\u001b[38;5;241m10.0\u001b[39m, \u001b[38;5;241m6.0\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m interact(plot_figure, n_iter\u001b[38;5;241m=\u001b[39mIntSlider(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mgd_ws\u001b[49m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gd_ws' is not defined"
     ]
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start stochastic gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "w_stoch, loss_stoch = mean_squared_error_sgd(y_train, x_train, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "print(\"Stochastic Gradient Descent: loss={l}, w={w}\".format(l=loss_stoch, w=w_stoch))\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
